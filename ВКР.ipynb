{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd3MIiXI-6AT",
        "outputId": "13e8ad31-2c1f-4df8-e64c-502ff68715cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.152-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.152-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.152 ultralytics-thop-2.0.14\n"
          ]
        }
      ],
      "source": [
        "pip install ultralytics opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tesseract-ocr-rus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Grl1r64AwJd",
        "outputId": "1f26b4e5-22ca-4982-a448-8921f3e71840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr-rus\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,271 kB of archives.\n",
            "After this operation, 3,877 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-rus all 1:4.00~git30-7274cfa-1.1 [1,271 kB]\n",
            "Fetched 1,271 kB in 1s (2,013 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-rus.\n",
            "(Reading database ... 126111 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-rus_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-rus (1:4.00~git30-7274cfa-1.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rapidfuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6OloWxgH_zS",
        "outputId": "c402e352-8bf2-4798-cbae-48d2af43e04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4VI3avsA0zW",
        "outputId": "793b7309-6cdf-4068-8c6c-dae9ee7a3f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.2.1)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU2IDI8WC_ze",
        "outputId": "3dcf2a45-15cb-4cbb-e826-57908f6c70ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install symspellpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4nEWVTWC5sJ",
        "outputId": "5c4b7dd2-cbf1-47a7-9008-b62c05b7ec43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.9.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Downloading symspellpy-6.9.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.6-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.4/158.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.6 symspellpy-6.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import openpyxl\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "import fitz  # PyMuPDF"
      ],
      "metadata": {
        "id": "KblCNxtCA6Vb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257e08f5-207b-4d75-dccf-b0b642abe82c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**В model необходимо добавить модель, в нашем случае обученные\n",
        "ранее веса модели yolov8(необходимо добавить путь к файлу best_yolov8.pt)**"
      ],
      "metadata": {
        "id": "Mp2WbexOb8BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"/content/drive/MyDrive/данные_вкр/best_yolov8.pt\")"
      ],
      "metadata": {
        "id": "pQvoLTCVBFIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Функция pdf_to_jpg переводит pdf документы в формат JPG, чтобы далее подать изображения нашему**\n",
        "\n",
        "Сначала создайте папку и поместите в нее pdf файлы(хотя бы один для примера)\n",
        "\n",
        "*pdf_folder* = Укажите путь к папке с PDF\n",
        "\n",
        "*output_folder* = Укажите путь для сохранения JPG"
      ],
      "metadata": {
        "id": "3WlE0D-3cokF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_jpg(pdf_path, output_folder, dpi=300):\n",
        "    \"\"\"\n",
        "    Конвертирует PDF-файл в изображения JPG (по одной JPG на каждую страницу).\n",
        "\n",
        "    :param pdf_path: Путь к PDF-файлу.\n",
        "    :param output_folder: Папка для сохранения JPG-изображений.\n",
        "    :param dpi: Качество изображения (DPI). По умолчанию 300.\n",
        "    \"\"\"\n",
        "    # Проверяем, существует ли PDF\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"Файл {pdf_path} не найден!\")\n",
        "\n",
        "    # Создаем папку для выходных файлов, если её нет\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    print(f\"Конвертация PDF: {pdf_path}...\")\n",
        "\n",
        "    pdf_document = fitz.open(pdf_path)\n",
        "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "    for page_num in range(len(pdf_document)):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "\n",
        "        zoom = dpi / 72  # 72 - стандартный DPI для PDF\n",
        "        mat = fitz.Matrix(zoom, zoom)\n",
        "\n",
        "        pix = page.get_pixmap(matrix=mat)\n",
        "        img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "        output_filename = f\"{pdf_name}_page_{page_num + 1}.jpg\"\n",
        "        output_path = os.path.join(output_folder, output_filename)\n",
        "        img.save(output_path, \"JPEG\", quality=95)\n",
        "        print(f\"Сохранено: {output_filename}\")\n",
        "\n",
        "    pdf_document.close()\n",
        "    print(f\"Конвертация завершена! Изображения сохранены в: {output_folder}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_R6Sr9BTDN6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c636810-b182-44e3-a7d3-d490115c2bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Конвертация PDF: /content/drive/MyDrive/данные_вкр/12. 012-2023-ШК1100-СПС Спецификация.pdf...\n",
            "Сохранено: 12. 012-2023-ШК1100-СПС Спецификация_page_1.jpg\n",
            "Сохранено: 12. 012-2023-ШК1100-СПС Спецификация_page_2.jpg\n",
            "Сохранено: 12. 012-2023-ШК1100-СПС Спецификация_page_3.jpg\n",
            "Конвертация завершена! Изображения сохранены в: /content/photo_test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Данная функция process_and_crop_boxes детектирует необходимые участки изображения(столбцы таблицы), сохраняет их в папке, фильтрует наиболее вероятные и визуализирует нам выделенные области**\n",
        "\n",
        "*image_path* - укажите путь к изображению таблицы в формате jpg"
      ],
      "metadata": {
        "id": "0fvZaO0fdF5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Tuple\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def process_and_crop_boxes(\n",
        "    image_path: str,\n",
        "    model,\n",
        "    output_base_dir: str = \"/content/cropped_images\",\n",
        "    display_results: bool = True\n",
        ") -> Optional[List[Tuple[str, float, int]]]:\n",
        "    \"\"\"\n",
        "    Обрабатывает изображение, детектирует объекты, обрезает и сохраняет только объекты с максимальной уверенностью для каждого класса.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Путь к исходному изображению\n",
        "        model: Модель для детекции (YOLO)\n",
        "        output_base_dir (str): Базовая директория для сохранения\n",
        "        display_results (bool): Показывать ли результаты в matplotlib\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float, int]]: Список кортежей (путь_к_файлу, уверенность, класс)\n",
        "        или None, если объекты не найдены\n",
        "    \"\"\"\n",
        "    image_np = cv2.imread(image_path)\n",
        "    if image_np is None:\n",
        "        print(f\"Ошибка: не удалось загрузить изображение {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Детекция объектов\n",
        "    results = model.predict(source=image_path, save_crop=False)\n",
        "\n",
        "    if not results or not results[0].boxes:\n",
        "        print(\"No objects detected.\")\n",
        "        return None\n",
        "\n",
        "    boxes_data = results[0].boxes.data.tolist()\n",
        "\n",
        "    max_conf_boxes = {}  # ключ - class_id, значение - (xyxy, conf)\n",
        "\n",
        "    for *xyxy, conf, cls in boxes_data:\n",
        "        class_id = int(cls)\n",
        "        confidence = float(conf)\n",
        "\n",
        "        if class_id not in max_conf_boxes or confidence > max_conf_boxes[class_id][1]:\n",
        "            max_conf_boxes[class_id] = (xyxy, confidence)\n",
        "\n",
        "    saved_crops = []\n",
        "\n",
        "    # Обрабатываем только объекты с максимальной уверенностью для каждого класса\n",
        "    for class_id, (xyxy, conf) in max_conf_boxes.items():\n",
        "        x1, y1, x2, y2 = map(int, xyxy)\n",
        "        cropped_image = image_np[y1:y2, x1:x2]\n",
        "\n",
        "        class_dir = os.path.join(output_base_dir, f\"class_{class_id}\")\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "        output_path = os.path.join(class_dir, f\"crop_{class_id}.jpg\")\n",
        "        cv2.imwrite(output_path, cropped_image)\n",
        "        saved_crops.append((output_path, conf, class_id))\n",
        "\n",
        "        # Вывод информации\n",
        "        print(\n",
        "            f\"Объект класса {class_id}: уверенность={conf:.2f}\\n\"\n",
        "            f\"Координаты: x={x1}-{x2}, y={y1}-{y2}\\n\"\n",
        "            f\"Сохранено в: {output_path}\\n\"\n",
        "        )\n",
        "\n",
        "        # if display_results:\n",
        "        #     plt.figure(figsize=(10, 8))\n",
        "        #     plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
        "        #     plt.title(f'Класс: {class_id}, Уверенность: {conf:.2f}')\n",
        "        #     plt.show()\n",
        "\n",
        "    return saved_crops"
      ],
      "metadata": {
        "id": "URhejFZyT8T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Данная функция extract_text_from_images извлекает текст из каждой ячейки выбранного столбца и производит фильтрацию прямоугольных контуров**\n",
        "\n",
        "*folder_path_photo* - укажите путь к папке с изображениями определенного столбца"
      ],
      "metadata": {
        "id": "zArzb6NP3XR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_images(root_folder_path):\n",
        "    \"\"\"\n",
        "    Обрабатывает все изображения во вложенных папках root_folder_path,\n",
        "    извлекает текст и возвращает вложенный список с результатами.\n",
        "    \"\"\"\n",
        "    all_images_texts = []\n",
        "\n",
        "    top_level_folders = sorted([\n",
        "        f for f in os.listdir(root_folder_path)\n",
        "        if os.path.isdir(os.path.join(root_folder_path, f))\n",
        "    ])\n",
        "\n",
        "    for folder in top_level_folders:\n",
        "        folder_path = os.path.join(root_folder_path, folder)\n",
        "\n",
        "        for root, _, files in os.walk(folder_path):\n",
        "            # Сортируем изображения для последовательной обработки\n",
        "            image_files = sorted([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "            for filename in image_files:\n",
        "                filepath = os.path.join(root, filename)\n",
        "                image_texts = []\n",
        "\n",
        "                try:\n",
        "                    img = Image.open(filepath)\n",
        "                    width, height = img.size\n",
        "                    gray = img.convert('L')\n",
        "                    threshold = gray.point(lambda x: 0 if x < 150 else 255, '1')\n",
        "\n",
        "                    threshold_np = np.array(threshold)\n",
        "                    threshold_np = threshold_np.astype('uint8')\n",
        "\n",
        "                    contours, _ = cv2.findContours(threshold_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "                    if len(contours) < 3:\n",
        "                        for contour in contours:\n",
        "                            x, y, w, h = cv2.boundingRect(contour)\n",
        "                            cell_img = img.crop((x, y, x+w, y+h))\n",
        "                            text = pytesseract.image_to_string(\n",
        "                                cell_img,\n",
        "                                lang='rus+eng',\n",
        "                                config='--oem 3 --psm 6'\n",
        "                            ).strip()\n",
        "                            clean_text = ' '.join(text.split()).replace('\\x0c', '')\n",
        "                            image_texts.append(clean_text if clean_text else math.nan)\n",
        "                        all_images_texts.append(image_texts)\n",
        "                        continue\n",
        "\n",
        "                    areas = [cv2.contourArea(contour) for contour in contours]\n",
        "                    bboxes = [cv2.boundingRect(contour) for contour in contours]\n",
        "                    avg_area = sum(areas) / len(areas)\n",
        "\n",
        "                    for contour, (x, y, w, h) in zip(contours, bboxes):\n",
        "                        if cv2.contourArea(contour) >= avg_area * 0.7:\n",
        "                            cell_img = img.crop((x, y, x+w, y+h))\n",
        "                            text = pytesseract.image_to_string(\n",
        "                                cell_img,\n",
        "                                lang='rus+eng',\n",
        "                                config='--oem 3 --psm 6'\n",
        "                            ).strip()\n",
        "                            clean_text = ' '.join(text.split()).replace('\\x0c', '')\n",
        "                            image_texts.append(clean_text if clean_text else math.nan)\n",
        "\n",
        "                    all_images_texts.append(image_texts)\n",
        "\n",
        "                except Exception:\n",
        "                    image_texts.append(math.nan)\n",
        "                    all_images_texts.append(image_texts)\n",
        "\n",
        "                finally:\n",
        "                    try:\n",
        "                        os.remove(filepath)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "    all_images_texts = [lst[::-1] for lst in all_images_texts]\n",
        "\n",
        "    if len(all_images_texts) > 1:\n",
        "        all_images_texts[-1], all_images_texts[-2] = all_images_texts[-2], all_images_texts[-1]\n",
        "\n",
        "    print(\"Итоговый список текстов:\")\n",
        "    print(all_images_texts)\n",
        "\n",
        "    return all_images_texts"
      ],
      "metadata": {
        "id": "a7IevwUiQWX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Функция lists_to_excel_columns представляет информацию в EXCEL таблице**"
      ],
      "metadata": {
        "id": "TOZ9iNsWdPQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import os\n",
        "\n",
        "def lists_to_excel_columns(list_of_lists, column_names, excel_file_path, offset=5):\n",
        "    \"\"\"\n",
        "    Добавляет список списков в существующий Excel файл с отступом в указанное количество строк.\n",
        "    Если файл не существует - создает новый.\n",
        "\n",
        "    Аргументы:\n",
        "        list_of_lists (list): Список списков, где каждый внутренний список представляет данные столбца\n",
        "        column_names (list): Список названий столбцов\n",
        "        excel_file_path (str): Путь к Excel файлу\n",
        "        offset (int): Количество пустых строк между существующими и новыми данными (по умолчанию 5)\n",
        "    \"\"\"\n",
        "    if len(list_of_lists) != len(column_names):\n",
        "        print(len(list_of_lists))\n",
        "        raise ValueError(\"Количество внутренних списков должно совпадать с количеством названий столбцов.\")\n",
        "\n",
        "    # Создаем DataFrame из новых данных\n",
        "    max_len = max(len(inner_list) for inner_list in list_of_lists)\n",
        "    padded_lists = [lst + [''] * (max_len - len(lst)) for lst in list_of_lists]\n",
        "    new_data = {column_names[i]: padded_lists[i] for i in range(len(column_names))}\n",
        "    new_df = pd.DataFrame(new_data)\n",
        "\n",
        "    if not os.path.exists(excel_file_path):\n",
        "        new_df.to_excel(excel_file_path, index=False)\n",
        "        print(f\"Создан новый файл: {excel_file_path}\")\n",
        "        return\n",
        "\n",
        "    book = openpyxl.load_workbook(excel_file_path)\n",
        "    sheet = book.active\n",
        "\n",
        "    last_row = 1\n",
        "    for row in sheet.iter_rows():\n",
        "        if any(cell.value for cell in row):\n",
        "            last_row = row[0].row\n",
        "\n",
        "    start_row = last_row + offset + 1  # +1 потому что last_row - последняя заполненная строка\n",
        "\n",
        "    if last_row == 1:\n",
        "        for col_num, column_name in enumerate(column_names, 1):\n",
        "            sheet.cell(row=1, column=col_num, value=column_name)\n",
        "        start_row = 2  # Данные начинаем записывать со второй строки\n",
        "\n",
        "    for r_idx, row in enumerate(dataframe_to_rows(new_df, index=False, header=False), start_row):\n",
        "        for c_idx, value in enumerate(row, 1):\n",
        "            sheet.cell(row=r_idx, column=c_idx, value=value)\n",
        "\n",
        "    book.save(excel_file_path)\n",
        "    print(f\"Данные успешно добавлены в файл {excel_file_path} с отступом {offset} строк\")"
      ],
      "metadata": {
        "id": "dY2S7qjUhfNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Корректор на основе расстояния Левенштейна**"
      ],
      "metadata": {
        "id": "V3fGi-uLdI1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# import numpy as np\n",
        "# from collections.abc import Iterable\n",
        "\n",
        "# class SpellCorrector:\n",
        "#     def __init__(self, dictionary_file, threshold=0.5):\n",
        "#         self.dictionary = self.load_dictionary(dictionary_file)\n",
        "#         self.threshold = threshold\n",
        "\n",
        "#     def load_dictionary(self, file_path):\n",
        "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#             return [line.strip() for line in file if line.strip()]\n",
        "\n",
        "#     def levenshtein_distance(self, s1, s2):\n",
        "#         if len(s1) < len(s2):\n",
        "#             return self.levenshtein_distance(s2, s1)\n",
        "#         if len(s2) == 0:\n",
        "#             return len(s1)\n",
        "\n",
        "#         previous_row = range(len(s2) + 1)\n",
        "#         for i, c1 in enumerate(s1):\n",
        "#             current_row = [i + 1]\n",
        "#             for j, c2 in enumerate(s2):\n",
        "#                 insertions = previous_row[j + 1] + 1\n",
        "#                 deletions = current_row[j] + 1\n",
        "#                 substitutions = previous_row[j] + (c1 != c2)\n",
        "#                 current_row.append(min(insertions, deletions, substitutions))\n",
        "#             previous_row = current_row\n",
        "#         return previous_row[-1]\n",
        "\n",
        "#     def normalized_levenshtein(self, s1, s2):\n",
        "#         distance = self.levenshtein_distance(s1, s2)\n",
        "#         max_len = max(len(s1), len(s2))\n",
        "#         return distance / max_len if max_len > 0 else 0.0\n",
        "\n",
        "#     def correct(self, word):\n",
        "#         if not self.dictionary:\n",
        "#             return (word, 0.0, False)\n",
        "\n",
        "#         distances = [self.normalized_levenshtein(word, dict_word) for dict_word in self.dictionary]\n",
        "#         min_index = np.argmin(distances)\n",
        "#         min_distance = distances[min_index]\n",
        "\n",
        "#         if min_distance <= self.threshold:\n",
        "#             return (self.dictionary[min_index], min_distance, True)\n",
        "#         return (word, min_distance, False)\n",
        "\n",
        "# def correct_nested_list(nested_list, corrector):\n",
        "#     \"\"\"\n",
        "#     Рекурсивно применяет корректор ко всем элементам вложенного списка.\n",
        "#     Возвращает новую структуру с исправленными элементами.\n",
        "#     \"\"\"\n",
        "#     result = []\n",
        "#     for item in nested_list:\n",
        "#         if isinstance(item, str):\n",
        "#             corrected, dist, changed = corrector.correct(item)\n",
        "#             result.append(corrected)\n",
        "#         elif isinstance(item, Iterable) and not isinstance(item, str):\n",
        "#             result.append(correct_nested_list(item, corrector))\n",
        "#         else:\n",
        "#             result.append(item)  # Для не-строк и не-итерируемых объектов\n",
        "#     return result"
      ],
      "metadata": {
        "id": "XESTL2RN4Pge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Корректор на основе расстояния Левенштейна ()оптимизированная версия**"
      ],
      "metadata": {
        "id": "RjdAiJKAc_8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import defaultdict\n",
        "# from functools import lru_cache\n",
        "# import numpy as np\n",
        "# from collections.abc import Iterable\n",
        "# import hashlib\n",
        "# import random\n",
        "# from typing import List, Union, Dict, Set, Tuple\n",
        "\n",
        "# class UltraFastSpellCorrector:\n",
        "#     def __init__(self, dictionary_file: str, threshold: float = 0.6):\n",
        "#         self.dictionary: Set[str] = set()\n",
        "#         self.word_freq: Dict[str, int] = defaultdict(int)\n",
        "#         self.length_index: Dict[int, List[str]] = defaultdict(list)\n",
        "#         self.prefix_index: Dict[str, List[str]] = defaultdict(list)\n",
        "#         self.word_hashes: Dict[str, str] = {}\n",
        "#         self.threshold = threshold\n",
        "#         self.max_prefix_len = 3  # Инициализируем до использования\n",
        "#         self._load_and_index_dictionary(dictionary_file)\n",
        "\n",
        "#         # Оптимизированные структуры\n",
        "#         self.sorted_by_freq: List[Tuple[str, int]] = []\n",
        "\n",
        "#     def _load_and_index_dictionary(self, file_path: str) -> None:\n",
        "#         \"\"\"Загружает словарь и строит все индексы\"\"\"\n",
        "#         with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#             for line in file:\n",
        "#                 word = line.strip()\n",
        "#                 if not word:\n",
        "#                     continue\n",
        "\n",
        "#                 self.dictionary.add(word)\n",
        "#                 self.word_freq[word] += 1\n",
        "#                 self.length_index[len(word)].append(word)\n",
        "\n",
        "#                 # Индекс по префиксам\n",
        "#                 prefix = word[:self.max_prefix_len].lower()\n",
        "#                 self.prefix_index[prefix].append(word)\n",
        "\n",
        "#                 # Предварительный расчет хэшей\n",
        "#                 self.word_hashes[word] = self._compute_hash(word)\n",
        "\n",
        "#         # Сортируем слова по частоте\n",
        "#         self.sorted_by_freq = sorted(self.word_freq.items(),\n",
        "#                                    key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _compute_hash(word: str) -> str:\n",
        "#         \"\"\"Вычисляет быстрый хэш для слова\"\"\"\n",
        "#         return hashlib.md5(word.encode('utf-8')).hexdigest()\n",
        "\n",
        "#     @lru_cache(maxsize=100000)\n",
        "#     def levenshtein_distance(self, s1: str, s2: str) -> int:\n",
        "#         \"\"\"Оптимизированное расстояние Левенштейна с кэшированием\"\"\"\n",
        "#         if len(s1) < len(s2):\n",
        "#             return self.levenshtein_distance(s2, s1)\n",
        "#         if len(s2) == 0:\n",
        "#             return len(s1)\n",
        "\n",
        "#         previous_row = list(range(len(s2) + 1))\n",
        "#         for i, c1 in enumerate(s1):\n",
        "#             current_row = [i + 1]\n",
        "#             for j, c2 in enumerate(s2):\n",
        "#                 insertions = previous_row[j + 1] + 1\n",
        "#                 deletions = current_row[j] + 1\n",
        "#                 substitutions = previous_row[j] + (c1 != c2)\n",
        "#                 current_row.append(min(insertions, deletions, substitutions))\n",
        "#             previous_row = current_row\n",
        "#         return previous_row[-1]\n",
        "\n",
        "#     def normalized_levenshtein(self, s1: str, s2: str) -> float:\n",
        "#         \"\"\"Нормализованное расстояние с оптимизациями\"\"\"\n",
        "#         # Быстрая проверка по хэшам\n",
        "#         if self.word_hashes.get(s1) == self.word_hashes.get(s2):\n",
        "#             return 0.0\n",
        "\n",
        "#         distance = self.levenshtein_distance(s1, s2)\n",
        "#         max_len = max(len(s1), len(s2))\n",
        "#         return distance / max_len if max_len > 0 else 0.0\n",
        "\n",
        "#     def get_candidates(self, word: str, max_candidates: int = 200) -> List[str]:\n",
        "#         \"\"\"Многоуровневый отбор кандидатов с приоритезацией\"\"\"\n",
        "#         candidates = set()\n",
        "#         target_len = len(word)\n",
        "#         target_prefix = word[:self.max_prefix_len].lower()\n",
        "\n",
        "#         # 1. Слова с таким же префиксом\n",
        "#         if target_prefix in self.prefix_index:\n",
        "#             candidates.update(self.prefix_index[target_prefix])\n",
        "\n",
        "#         # 2. Слова аналогичной длины (±1 символ)\n",
        "#         for l in range(max(1, target_len-1), target_len+2):\n",
        "#             if l in self.length_index:\n",
        "#                 candidates.update(self.length_index[l])\n",
        "\n",
        "#         # 3. Самые частые слова, если кандидатов мало\n",
        "#         if len(candidates) < max_candidates//2:\n",
        "#             top_words = [w for w, _ in self.sorted_by_freq[:max_candidates//2]]\n",
        "#             candidates.update(top_words)\n",
        "\n",
        "#         # 4. Если все еще мало, добавляем случайные слова\n",
        "#         if len(candidates) < max_candidates//4 and self.dictionary:\n",
        "#             candidates.update(random.sample(list(self.dictionary),\n",
        "#                              min(50, len(self.dictionary))))\n",
        "\n",
        "#         return list(candidates)[:max_candidates]\n",
        "\n",
        "#     def correct(self, word: str) -> Tuple[str, float, bool]:\n",
        "#         \"\"\"Ультраоптимизированная коррекция с многоуровневым отбором\"\"\"\n",
        "#         if not word:  # Проверка на пустую строку\n",
        "#             return (word, 1.0, False)\n",
        "\n",
        "#         if word in self.dictionary:\n",
        "#             return (word, 0.0, False)\n",
        "\n",
        "#         candidates = self.get_candidates(word)\n",
        "#         if not candidates:\n",
        "#             return (word, 1.0, False)\n",
        "\n",
        "#         # Быстрая проверка точных совпадений после хэшей\n",
        "#         word_hash = self._compute_hash(word)\n",
        "#         for candidate in candidates:\n",
        "#             if self.word_hashes.get(candidate) == word_hash:\n",
        "#                 return (candidate, 0.0, True)\n",
        "\n",
        "#         # Вычисляем расстояния только для отобранных кандидатов\n",
        "#         min_dist = float('inf')\n",
        "#         best_candidate = word\n",
        "\n",
        "#         for candidate in candidates:\n",
        "#             dist = self.normalized_levenshtein(word, candidate)\n",
        "#             if dist < min_dist:\n",
        "#                 min_dist = dist\n",
        "#                 best_candidate = candidate\n",
        "#                 if min_dist == 0:  # Нашли точное совпадение\n",
        "#                     break\n",
        "\n",
        "#         if min_dist <= self.threshold:\n",
        "#             return (best_candidate, min_dist, True)\n",
        "#         return (word, min_dist, False)\n",
        "\n",
        "# def correct_nested_list(nested_list: List[Union[str, Iterable]],\n",
        "#                        corrector: UltraFastSpellCorrector) -> List[Union[str, Iterable]]:\n",
        "#     \"\"\"\n",
        "#     Рекурсивная коррекция для сложных структур данных\n",
        "#     \"\"\"\n",
        "#     result = []\n",
        "#     for item in nested_list:\n",
        "#         if isinstance(item, str):\n",
        "#             corrected, dist, changed = corrector.correct(item)\n",
        "#             result.append(corrected)\n",
        "#         elif isinstance(item, Iterable) and not isinstance(item, str):\n",
        "#             result.append(correct_nested_list(item, corrector))\n",
        "#         else:\n",
        "#             result.append(item)\n",
        "#     return result"
      ],
      "metadata": {
        "id": "-qAhDX5LiVQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Корректор на основе SymSpellpy**"
      ],
      "metadata": {
        "id": "i0_DSiY0c4w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from symspellpy import SymSpell, Verbosity\n",
        "# from typing import List, Union\n",
        "# import re\n",
        "\n",
        "# class SymSpellCorrector:\n",
        "#     def __init__(self, dict_path: str, max_edit_distance: int = 2):\n",
        "#         self.sym_spell = SymSpell(max_dictionary_edit_distance=max_edit_distance)\n",
        "#         self.load_dictionary(dict_path)\n",
        "\n",
        "#     def load_dictionary(self, dict_path: str):\n",
        "#         \"\"\"Загрузка пользовательского словаря из txt файла\"\"\"\n",
        "#         # Формат: слово частота (частота может быть произвольной, например 1)\n",
        "#         self.sym_spell.load_dictionary(dict_path, term_index=0, count_index=1)\n",
        "\n",
        "#     def correct_word(self, word: str) -> str:\n",
        "#         \"\"\"Коррекция отдельного слова\"\"\"\n",
        "#         suggestions = self.sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
        "#         return suggestions[0].term if suggestions else word\n",
        "\n",
        "#     def correct_text(self, text: str) -> str:\n",
        "#         \"\"\"Коррекция текста с сохранением регистра и пунктуации\"\"\"\n",
        "#         words = re.findall(r\"(\\w+|\\W+)\", text)  # Разделяем на слова и не-слова\n",
        "#         corrected_words = []\n",
        "\n",
        "#         for token in words:\n",
        "#             if token.isalpha():  # Если токен - слово (только буквы)\n",
        "#                 original_case = token\n",
        "#                 lower_token = token.lower()\n",
        "#                 corrected = self.correct_word(lower_token)\n",
        "\n",
        "#                 # Восстанавливаем оригинальный регистр первой буквы\n",
        "#                 if original_case.istitle():\n",
        "#                     corrected = corrected.capitalize()\n",
        "#                 elif original_case.isupper():\n",
        "#                     corrected = corrected.upper()\n",
        "\n",
        "#                 corrected_words.append(corrected)\n",
        "#             else:\n",
        "#                 corrected_words.append(token)  # Пробелы, пунктуация и др. оставляем как есть\n",
        "\n",
        "#         return \"\".join(corrected_words)\n",
        "\n",
        "#     def correct_nested_list(self, nested_list: List[Union[List, str]]) -> List[Union[List, str]]:\n",
        "#         \"\"\"Рекурсивная обработка вложенных списков\"\"\"\n",
        "#         corrected_list = []\n",
        "#         for item in nested_list:\n",
        "#             if isinstance(item, str):\n",
        "#                 corrected_list.append(self.correct_text(item))\n",
        "#             elif isinstance(item, list):\n",
        "#                 corrected_list.append(self.correct_nested_list(item))\n",
        "#             else:\n",
        "#                 corrected_list.append(item)  # На случай, если есть не строки и не списки\n",
        "#         return corrected_list\n",
        "\n",
        "\n",
        "# # # Пример использования\n",
        "# # if __name__ == \"__main__\":\n",
        "# #     # Инициализация с вашим словарем\n",
        "# #     corrector = SymSpellCorrector(\"my_dict.txt\")  # Формат словаря: слово частота\n",
        "\n",
        "# #     # Пример вложенного списка\n",
        "# #     nested_texts = [\n",
        "# #         \"Привет, как твои дела?\",\n",
        "# #         [\"Это тестввый пример\", [\"с ошшибками\", \"и вложенными списками\"]],\n",
        "# #         \"Последняя стррока с ошибками\"\n",
        "# #     ]\n",
        "\n",
        "# #     # Коррекция\n",
        "# #     corrected_texts = corrector.correct_nested_list(nested_texts)\n",
        "\n",
        "# #     # Вывод результатов\n",
        "# #     print(\"До коррекции:\")\n",
        "# #     print(nested_texts)\n",
        "# #     print(\"\\nПосле коррекции:\")\n",
        "# #     print(corrected_texts)"
      ],
      "metadata": {
        "id": "LsxcxvKMeFf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Укажите путь к PDF**"
      ],
      "metadata": {
        "id": "DebEK30fcix_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/drive/MyDrive/данные_вкр/01-02-2021-ПС.ДУ5.СО2.pdf\"  # Укажите путь к PDF\n",
        "output_folder = \"/content/photo_test\"  # Папка для сохранения JPG"
      ],
      "metadata": {
        "id": "E5Bogpu_cY0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Далее, запустите следующую ячейку на выходе получаем EXCEL документ**"
      ],
      "metadata": {
        "id": "MyYQr08WcnTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "pdf_to_jpg(pdf_path, output_folder)\n",
        "\n",
        "def process_all_images_in_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Основная функция для обработки всех изображений в папке\n",
        "    Удаляет все изображения после успешной обработки\n",
        "    \"\"\"\n",
        "    files = os.listdir(folder_path)\n",
        "\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
        "    image_files = [f for f in files if os.path.splitext(f)[1].lower() in image_extensions]\n",
        "\n",
        "    if not image_files:\n",
        "        print(\"В папке нет изображений\")\n",
        "        return\n",
        "\n",
        "    image_files.sort()\n",
        "\n",
        "    try:\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(folder_path, image_file)\n",
        "            print(f\"\\nНачало обработки: {image_file}\")\n",
        "\n",
        "            # Обработка изображения и выделение боксов\n",
        "            process_and_crop_boxes(\n",
        "                image_path=image_path,\n",
        "                model=model,\n",
        "                output_base_dir=\"/content/cropped_images\",\n",
        "                display_results=True\n",
        "            )\n",
        "\n",
        "            # Извлечение текста из обработанных изображений\n",
        "            root_folder = '/content/cropped_images'\n",
        "            extracted_text = extract_text_from_images(root_folder)\n",
        "\n",
        "            # corrector = UltraFastSpellCorrector(\"/content/text_from_db_полный.txt\",threshold=0.5)\n",
        "            # corrected_nested = correct_nested_list(extracted_text, corrector)\n",
        "            # corrector = SymSpellCorrector(\"/content/tech_dictionary.txt\")\n",
        "            # corrected_texts = corrector.correct_nested_list(extracted_text)\n",
        "\n",
        "\n",
        "            # Преобразование в колонки Excel\n",
        "            column_names = [\"Наименование затрат\", \"Примечание\", \"Ед. изм.\", \"Кол-во\"]\n",
        "            table_task = lists_to_excel_columns(\n",
        "                list_of_lists=extracted_text,\n",
        "                column_names=column_names,\n",
        "                excel_file_path='/content/sales.xlsx',\n",
        "                offset=5\n",
        "            )\n",
        "\n",
        "            print(f\"Обработка {image_file} завершена\")\n",
        "\n",
        "        # После успешной обработки всех изображений - очищаем папку\n",
        "        print(\"\\nУдаление обработанных изображений...\")\n",
        "        for filename in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "                    os.unlink(file_path)\n",
        "                elif os.path.isdir(file_path):\n",
        "                    shutil.rmtree(file_path)\n",
        "            except Exception as e:\n",
        "                print(f'Ошибка при удалении {file_path}. Причина: {e}')\n",
        "\n",
        "        print(f\"Все изображения из {folder_path} успешно удалены\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Произошла ошибка во время обработки: {e}\")\n",
        "        print(\"Изображения не были удалены из-за ошибки обработки\")\n",
        "\n",
        "# Укажите путь к папке с изображениями\n",
        "folder_path = '/content/photo_test'\n",
        "\n",
        "# Запускаем обработку всех изображений в папке\n",
        "process_all_images_in_folder(folder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCs8t8HtWG8w",
        "outputId": "79b1acff-5aec-4933-f2fa-b687cb981627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Конвертация PDF: /content/drive/MyDrive/данные_вкр/01-02-2021-ПС.ДУ5.СО2.pdf...\n",
            "Сохранено: 01-02-2021-ПС.ДУ5.СО2_page_1.jpg\n",
            "Сохранено: 01-02-2021-ПС.ДУ5.СО2_page_2.jpg\n",
            "Сохранено: 01-02-2021-ПС.ДУ5.СО2_page_3.jpg\n",
            "Конвертация завершена! Изображения сохранены в: /content/photo_test\n",
            "\n",
            "Начало обработки: 01-02-2021-ПС.ДУ5.СО2_page_1.jpg\n",
            "\n",
            "image 1/1 /content/photo_test/01-02-2021-ПС.ДУ5.СО2_page_1.jpg: 480x640 1 Name of costs, 1 Note, 1 Quantity, 2 Unit of measurements, 282.2ms\n",
            "Speed: 10.2ms preprocess, 282.2ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Объект класса 0: уверенность=1.00\n",
            "Координаты: x=474-2009, y=462-2678\n",
            "Сохранено в: /content/cropped_images/class_0/crop_0.jpg\n",
            "\n",
            "Объект класса 1: уверенность=0.98\n",
            "Координаты: x=2005-2718, y=445-2663\n",
            "Сохранено в: /content/cropped_images/class_1/crop_1.jpg\n",
            "\n",
            "Объект класса 2: уверенность=0.98\n",
            "Координаты: x=3897-4133, y=423-2674\n",
            "Сохранено в: /content/cropped_images/class_2/crop_2.jpg\n",
            "\n",
            "Объект класса 3: уверенность=0.94\n",
            "Координаты: x=3662-3897, y=414-2673\n",
            "Сохранено в: /content/cropped_images/class_3/crop_3.jpg\n",
            "\n",
            "Итоговый список текстов:\n",
            "[[nan, '1 Приборы и средства автоматизации', '1.1 Оборудование в электрощитовой', 'Прибор приемно-контрольный охранно-пожарный адресный', 'Блок индикации и управления', 'Релейный модуль', 'Релейный модуль', 'Источник вторичного электропитания 24В, 2,5А', 'Аккумулятор, 128, 12 А/ч', nan, '1.2 Прочее оборудование', 'Модуль управления клапаном', 'Релейный модуль', 'Релейный модуль', 'Метка адресная пожарная', nan, 'Изолятор шлейфа', 'Устройство коммутационное: 2 реле, контакты на переключение', nan, '1.3 Извещатели', 'Извещатель пожарный дымовой оптико-электронный адресно- аналоговый', 'Извещатель пожарный ручной адресный со встроенным изолятором короткого замыкания'], [nan, 'АВ', 'ЖИ', 'R3-Py6ex-20N', 'В3-Рубеж-БИУ', 'РМ-4 прот. R3', 'РМ-4К прот. R3', 'UBSNP 24/2,5 RS-R3 2x12 BP', 'и', 'АВ', nan, 'МДУ-1 прот. КЗ', 'РМ-4К прот. R3', 'РМ-4 прот. R3', 'АМП-4 прот. R3', 'АВ', 'ИЗ-1 прот. ВЗ', 'YK-BK/02', nan, nan, 'UN 212-64-R3 ПАСН.425232.038', 'ИПР 513-11 ИКЗ-А-БЗ'], [nan, nan, nan, 'LUT', 'LUT', 'LUT', 'LUT', 'LUT', 'шт', nan, nan, 'шт', 'LUT', 'шт', 'LUT', nan, 'LUT', 'LUT', nan, nan, 'LUT', 'LUT'], ['о', nan, 'о', '1', '1', '1', '3', '3', '6', 'о', 'о', '31', '9', '1', '1', 'о', '186', '3', 'о', 'о', '245', '28']]\n",
            "Создан новый файл: /content/sales.xlsx\n",
            "Обработка 01-02-2021-ПС.ДУ5.СО2_page_1.jpg завершена\n",
            "\n",
            "Начало обработки: 01-02-2021-ПС.ДУ5.СО2_page_2.jpg\n",
            "\n",
            "image 1/1 /content/photo_test/01-02-2021-ПС.ДУ5.СО2_page_2.jpg: 480x640 1 Name of costs, 1 Note, 1 Quantity, 1 Unit of measurement, 177.9ms\n",
            "Speed: 4.1ms preprocess, 177.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Объект класса 0: уверенность=1.00\n",
            "Координаты: x=475-2010, y=438-3011\n",
            "Сохранено в: /content/cropped_images/class_0/crop_0.jpg\n",
            "\n",
            "Объект класса 1: уверенность=0.99\n",
            "Координаты: x=2005-2713, y=447-3009\n",
            "Сохранено в: /content/cropped_images/class_1/crop_1.jpg\n",
            "\n",
            "Объект класса 2: уверенность=0.98\n",
            "Координаты: x=3896-4135, y=397-3075\n",
            "Сохранено в: /content/cropped_images/class_2/crop_2.jpg\n",
            "\n",
            "Объект класса 3: уверенность=0.88\n",
            "Координаты: x=3662-3894, y=400-3055\n",
            "Сохранено в: /content/cropped_images/class_3/crop_3.jpg\n",
            "\n",
            "Итоговый список текстов:\n",
            "[['Устройство дистанционного пуска адресное (дымоудаление) со встроенным изолятором короткого замыкания', 'Устройство дистанционного пуска адресное (пожаротушение)', 'Извещатель пожарный дымовой автономный', nan, '1.4 Оповещатели пожарные', 'Оповещатель охранно-пожарный звуковой 24В, 20мА, 105дБ', 'Оповещатель охранно-пожарный световой (табло)', nan, nan, '1.5 Шкафы управления', 'Шкаф управления адресный', 'Шкаф управления адресный', 'Шкаф управления адресный', nan, '2 Кабельные изделия', 'ПожТехКабель-КПСнг(А)-РЕЕ$ 1х2х0,5 Кабель огнестойкий, безга- логенный с низким газо- дымовыделением для систем охранно- пожарной сигнализации (ОПС) и систем оповещения и управления эвакуацией (СОУЭ), сохраняющие работоспособность в условиях открытого пламени в течении 180мин. (проводник медный 1 класса) на напряжение 300/5008В', 'ПожТехКабель-КПСнг(А)-РЕЕ$ 1х2х1,5 Кабель огнестойкий, безга- логенный с низким газо- дымовыделением для систем охранно- пожарной сигнализации (ОПС) и систем оповещения и управления эвакуацией (СОУЭ), сохраняющие работоспособность в условиях открытого пламени в течении 180мин. (проводник медный 1 класса) на напряжение 300/5008В', 'ПожТехКабель-КПСнг(А)-РЕЕ$ 1х2х0,75 Кабель огнестойкий, безга- логенный с низким газо- дымовыделением для систем охранно- пожарной сигнализации (ОПС) и систем оповещения и управления эвакуацией (СОУЭ), сохраняющие работоспособность в условиях открытого пламени в течении 180мин. (проводник медный 1 класса) на напряжение 300/5008В'], ['УДП 513-11 ИКЗ- ВЗ', 'УДП 513-11 ИКЗ- ВЗ', 'ИП-212-142', 'ВЫ', 'АВ', 'Maak-24-3M1', 'КРИСТАЛЛ-24 \"Выход\"', nan, 'АВ', nan, 'LUYH/B-15-YNN-R3', 'LUYH/B-3,0-R3', 'LUYH/B-3,0-R3', 'ВЫ', 'АВ', 'КПСнг(А)-РРЕ$ 1х2х0,5', 'КПСнг(А)-РРЕ$ 1х2х1,5', 'КПСнгА)-РРЕ$ 1х2х0,75'], ['LUT', 'шт', 'шт', nan, nan, 'LUT', 'шт', nan, nan, nan, 'LUT', 'шт', 'шт', nan, nan, 'м', 'м', 'м', nan], ['15', '32', '345', 'о', nan, '40', '45', 'о', 'о', 'о', '1', '1', '1', 'о', nan, '3105', '350', '415', 'о']]\n",
            "Данные успешно добавлены в файл /content/sales.xlsx с отступом 5 строк\n",
            "Обработка 01-02-2021-ПС.ДУ5.СО2_page_2.jpg завершена\n",
            "\n",
            "Начало обработки: 01-02-2021-ПС.ДУ5.СО2_page_3.jpg\n",
            "\n",
            "image 1/1 /content/photo_test/01-02-2021-ПС.ДУ5.СО2_page_3.jpg: 480x640 1 Name of costs, 1 Note, 1 Quantity, 1 Unit of measurement, 251.0ms\n",
            "Speed: 5.7ms preprocess, 251.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Объект класса 0: уверенность=0.99\n",
            "Координаты: x=473-2008, y=534-2880\n",
            "Сохранено в: /content/cropped_images/class_0/crop_0.jpg\n",
            "\n",
            "Объект класса 2: уверенность=0.97\n",
            "Координаты: x=3898-4136, y=459-2918\n",
            "Сохранено в: /content/cropped_images/class_2/crop_2.jpg\n",
            "\n",
            "Объект класса 3: уверенность=0.95\n",
            "Координаты: x=3662-3896, y=482-2896\n",
            "Сохранено в: /content/cropped_images/class_3/crop_3.jpg\n",
            "\n",
            "Объект класса 1: уверенность=0.38\n",
            "Координаты: x=2003-2708, y=430-2887\n",
            "Сохранено в: /content/cropped_images/class_1/crop_1.jpg\n",
            "\n",
            "Итоговый список текстов:\n",
            "[[nan, nan, '3 Монтажные материалы', 'Коробка монтажная огнестойкая', 'Коробка монтажная огнестойкая', 'Труба гофрированная с зондом 20 мм.из ПНД, трудногорючая, без- галогенная ЕКНЕ гибкая со стальной протяжкой, цвет черный. Пож- ТехКабель', 'Скоба металлическая однолапковая 19-20 мм под саморез (100шт упаковка) ПожТехКабель РТК-Ассеззопез', 'Дюбель металлический 5х30мм (500 шт/уп) ПожТехКабель РТК- Accessories', 'Саморез 3,5х35 мм (1000 шт/уп) ПожТехКабель РТК-Ассеззопез', 'Труба ПВХ диам. 50мм', 'Хомут для труб диам. 50мм', nan, 'Огнестойкая кабельная линия Райап+Есор!а${ ОКЛ-1 в составе:', 'Труба НЕК легкая с зондом гибкая гофрированная, из композиции полиолифенов (без галогена), самозатухающая @ 20 мм', 'Скоба оцинкованная с одним отверстием, для трубы 020 мм (100шт упаковка)', 'Универсальный металлический дюбель МУ 5/30 (500шт упаковка)', 'Шуруп стальной оцинкованный, шлиц РН 4,8х32мм 01№7981 (500шт упаковка)', nan, '4 Материалы для замоноличивания', 'Коробка распределительная', 'Коробка распределительная', 'Трубопровод из гибких тяжелых гофрированных труб Ву25мм'], ['ParLan U/UTP Cat5e PVCLS HIr(A)-FRLS 4x2x0,52', nan, 'И', 'КМОМ 75х75х37', 'KMOM 210x100x37', nan, nan, nan, 'И', 'ЖИ', 'И', nan, nan, nan, nan, 'И', 'и', nan, nan, 'Л250У3', 'Л251УЗ', nan, nan], ['rr', nan, nan, 'шт', 'шт', 'м', 'шт', 'шт', 'LUT', 'м', 'LUT', nan, nan, 'м', 'шт', 'шт', 'шт', nan, nan, 'шт', 'шт', 'м', nan], ['145', 'о', 'о', '27', '31', '835', '21', '5', '3', '350', '300', 'о', nan, '145', '4', '1', '1', 'о', 'о', '261', '319', '1230', '|']]\n",
            "Данные успешно добавлены в файл /content/sales.xlsx с отступом 5 строк\n",
            "Обработка 01-02-2021-ПС.ДУ5.СО2_page_3.jpg завершена\n",
            "\n",
            "Удаление обработанных изображений...\n",
            "Все изображения из /content/photo_test успешно удалены\n"
          ]
        }
      ]
    }
  ]
}